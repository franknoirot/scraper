# A Node XML Sitemap Scraper Prototype
*for converting site data to JSON, powered by Puppeteer and sitemap.js*

## How to use
1. Download .xml sitemap file(s) from site of your choice and place in the `/xml/` directory
2. List out the directories you want to crawl `directoryCallbacks` array at the top of `index.js`, using the pre- and post-callback functionality if your directory doesn't follow strict rules.
3. write a function for each type of page you want to crawl within `./scrapeFns.js` and set it as the `callback` within the page type's config object in `directoryCallbacks`, using some helper functions I've written over Puppeteer. Use the example callbacks as inspiration.
4. delete the `/site/` directory: this will be generated by the script and you don't want it partially overwritten with the example data in there.
5. run the script with `npm run scrape` and watch the `/site/` directory populate!

## Roadmap
I'd like to create a static frontend that lets you drop in the sitemap files, then build up the crawling rules through a UI. This will probably eventually become an Electron app. Here are some of the tricky problems I need to solve and thoughts on how I'll do it:

### How to create crawl functions through UI?
I think this will have to be a kind of visual inspector eventually, so that someone can select the data they want and fine-tune the selector from there. Will probably use an iFrame.

### How to set conditional callbacks (preCallback branching logic) with UI?
No idea, starting to sound like a node-based editor, which sounds super hard.